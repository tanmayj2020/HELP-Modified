{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Latency.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP108l7g2uel6s2mgdcDM+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanmayj2020/HELP-Modified/blob/master/Latency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTecPFscYSCv",
        "outputId": "a1771ec0-f993-44a2-8e4f-24c54c6b5733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 26 08:31:16 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SINGLE ARCHITECTURE - SINGLE HARDWARE"
      ],
      "metadata": {
        "id": "8r8PPsvQZGZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import copy "
      ],
      "metadata": {
        "id": "enGLhU5bdFPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_latency_gpu(model , dummy_input):\n",
        "  device = torch.device(\"cuda\")\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  dummy_input = dummy_input.to(device)\n",
        "  starter , ender = torch.cuda.Event(enable_timing = True) , torch.cuda.Event(enable_timing=True)\n",
        "  repetitions = 300\n",
        "  timings =np.zeros((repetitions , 1))\n",
        "  #GPU WARM UP \n",
        "  for _ in range(10):\n",
        "    _ = model(dummy_input)\n",
        "  #MEASURE LATENCY \n",
        "  with torch.no_grad():\n",
        "    for rep in range(repetitions):\n",
        "      starter.record()\n",
        "      _ = model(dummy_input) \n",
        "      ender.record()\n",
        "      #WAIT for GPU sync \n",
        "      torch.cuda.synchronize()\n",
        "      curr_time = starter.elapsed_time(ender)\n",
        "      timings[rep] = curr_time\n",
        "  mean_syn = np.sum(timings)/repetitions\n",
        "  std_syn = np.std(timings)\n",
        "  return mean_syn , std_syn\n",
        "  \n",
        "def calculate_latency_cpu(model , dummy_input):\n",
        "  repetitions = 2\n",
        "  model.eval()\n",
        "  timings= np.zeros((repetitions , 1))\n",
        "  #WARM UP \n",
        "  #for _ in range(10):\n",
        "   # _ = model(dummy_input)\n",
        "  #MEASURE LATENCY \n",
        "  with torch.no_grad():\n",
        "    for rep in range(repetitions):\n",
        "      start_time = time.time()\n",
        "      _ = model(dummy_input) \n",
        "      end_time = time.time()\n",
        "      curr_time = end_time -start_time\n",
        "      timings[rep] = curr_time\n",
        "  mean_syn = np.sum(timings)/repetitions\n",
        "  std_syn = np.std(timings)\n",
        "  return mean_syn , std_syn\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "dVmNoEakZNL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_input():\n",
        "  torch.manual_seed(1)\n",
        "  dummy_input = torch.randn(1 , 3 , 224 , 224 , dtype=torch.float)\n",
        "  return dummy_input"
      ],
      "metadata": {
        "id": "Fpmqf42Nc49j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alexnet1():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model1 = copy.deepcopy(model) \n",
        "  model1.features[3] = nn.Conv2d(64,100, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model1.features[6] = nn.Conv2d(100,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model1\n",
        "\n",
        "def alexnet2():\n",
        "  model  = torchvision.models.alexnet(pretrained=True) \n",
        "  model2 = copy.deepcopy(model) \n",
        "  model2.features[3] = nn.Conv2d(64,90, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model2.features[6] = nn.Conv2d(90,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model2\n",
        "\n",
        "def alexnet3():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model2 = copy.deepcopy(model) \n",
        "  model2.features[3] = nn.Conv2d(64,80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model2.features[6] = nn.Conv2d(80,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model2\n",
        "\n",
        "def alexnet4():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model3 = copy.deepcopy(model) \n",
        "  model3.features[3] = nn.Conv2d(64,70, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model3.features[6] = nn.Conv2d(70,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model3\n",
        "\n",
        "def alexnet5():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model4 = copy.deepcopy(model) \n",
        "  model4.features[3] = nn.Conv2d(64,105, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model4.features[6] = nn.Conv2d(105,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model4\n",
        "\n",
        "def alexnet6():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model5 = copy.deepcopy(model) \n",
        "  model5.features[3] = nn.Conv2d(64,120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model5.features[6] = nn.Conv2d(120,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model5\n",
        "  \n",
        "def alexnet7():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model6 = copy.deepcopy(model) \n",
        "  model6.features[3] = nn.Conv2d(64,130, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model6.features[6] = nn.Conv2d(130,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model6\n",
        "  \n",
        "def alexnet8():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model7 = copy.deepcopy(model) \n",
        "  model7.features[3] = nn.Conv2d(64,140, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model7.features[6] = nn.Conv2d(140,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model7\n",
        "  \n",
        "def alexnet9():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)  \n",
        "  model8 = copy.deepcopy(model) \n",
        "  model8.features[3] = nn.Conv2d(64,145, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model8.features[6] = nn.Conv2d(145,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model8\n",
        "  \n",
        "def alexnet10():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model9 = copy.deepcopy(model) \n",
        "  model9.features[3] = nn.Conv2d(64,160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model9.features[6] = nn.Conv2d(160,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model9\n",
        "  \n",
        "def alexnet11():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)  \n",
        "  model10 = copy.deepcopy(model) \n",
        "  model10.features[3] = nn.Conv2d(64,170, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model10.features[6] = nn.Conv2d(170,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model10\n",
        "  \n",
        "def alexnet12():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model11 = copy.deepcopy(model) \n",
        "  model11.features[3] = nn.Conv2d(64,150, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model11.features[6] = nn.Conv2d(150,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model11\n",
        "  \n",
        "def alexnet13():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)  \n",
        "  model12 = copy.deepcopy(model) \n",
        "  model12.features[3] = nn.Conv2d(64,180, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model12.features[6] = nn.Conv2d(180,384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model12\n",
        "  \n",
        "def alexnet14():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model13 = copy.deepcopy(model) \n",
        "  model13.features[6] = nn.Conv2d(192,300, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model13.features[8] = nn.Conv2d(300,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model13\n",
        "  \n",
        "def alexnet15():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model14 = copy.deepcopy(model) \n",
        "  model14.features[6] = nn.Conv2d(192,310, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model14.features[8] = nn.Conv2d(310,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model14\n",
        "  \n",
        "def alexnet16():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model15 = copy.deepcopy(model) \n",
        "  model15.features[6] = nn.Conv2d(192,320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model15.features[8] = nn.Conv2d(320,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model15\n",
        "  \n",
        "def alexnet17():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model16 = copy.deepcopy(model) \n",
        "  model16.features[6] = nn.Conv2d(192,330, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model16.features[8] = nn.Conv2d(330,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model16\n",
        "  \n",
        "def alexnet18():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model17 = copy.deepcopy(model) \n",
        "  model17.features[6] = nn.Conv2d(192,340, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model17.features[8] = nn.Conv2d(340,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model17\n",
        "  \n",
        "def alexnet19():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model18 = copy.deepcopy(model) \n",
        "  model18.features[6] = nn.Conv2d(192,350, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model18.features[8] = nn.Conv2d(350,256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
        "  return model18\n",
        "  \n",
        "def alexnet20():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model19 = copy.deepcopy(model) \n",
        "  model19.features[6] = nn.Conv2d(192,360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model19.features[8] = nn.Conv2d(360,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model19\n",
        "  \n",
        "def alexnet21():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model20 = copy.deepcopy(model) \n",
        "  model20.features[6] = nn.Conv2d(192,370, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model20.features[8] = nn.Conv2d(370,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model20\n",
        "  \n",
        "def alexnet22():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model21 = copy.deepcopy(model) \n",
        "  model21.features[6] = nn.Conv2d(192,380, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model21.features[8] = nn.Conv2d(380,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model21\n",
        "  \n",
        "def alexnet23():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model22 = copy.deepcopy(model) \n",
        "  model22.features[6] = nn.Conv2d(192,200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model22.features[8] = nn.Conv2d(200,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model22\n",
        "  \n",
        "def alexnet24():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model23 = copy.deepcopy(model) \n",
        "  model23.features[6] = nn.Conv2d(192,210, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model23.features[8] = nn.Conv2d(210,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model23\n",
        "  \n",
        "def alexnet25():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model24 = copy.deepcopy(model) \n",
        "  model24.features[6] = nn.Conv2d(192,250, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model24.features[8] = nn.Conv2d(250,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model24\n",
        "  \n",
        "def alexnet26():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model25 = copy.deepcopy(model) \n",
        "  model25.features[6] = nn.Conv2d(192,220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model25.features[8] = nn.Conv2d(220,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model25\n",
        "  \n",
        "def alexnet27():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model26 = copy.deepcopy(model)\n",
        "  model26.features[0] = nn.Conv2d(3 , 100, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model26.features[3] = nn.Conv2d(100,150, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model26.features[6] = nn.Conv2d(150,200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model26.features[8] = nn.Conv2d(200,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model26\n",
        "  \n",
        "def alexnet28():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model27 = copy.deepcopy(model) \n",
        "  model27.features[0] = nn.Conv2d(3,90, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model27.features[3] = nn.Conv2d(90,150, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model27.features[6] = nn.Conv2d(150,240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model27.features[8] = nn.Conv2d(240,256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model27\n",
        "  \n",
        "def alexnet29():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model28 = copy.deepcopy(model) \n",
        "  model28.features[0] = nn.Conv2d(3,200, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model28.features[3] = nn.Conv2d(200,220, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model28.features[6] = nn.Conv2d(220,290,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model28.features[8] = nn.Conv2d(290,256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model28\n",
        "  \n",
        "def alexnet30():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model29 = copy.deepcopy(model) \n",
        "  model29.features[0] = nn.Conv2d(3,180,kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model29.features[3] = nn.Conv2d(180,360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model29.features[6] = nn.Conv2d(360,420, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model29.features[8] = nn.Conv2d(420,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model29\n",
        "  \n",
        "def alexnet31():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model30 = copy.deepcopy(model) \n",
        "  model30.features[0] = nn.Conv2d(3,70, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model30.features[3] = nn.Conv2d(70,150, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model30.features[6] = nn.Conv2d(150,210, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model30.features[8] = nn.Conv2d(210,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model30\n",
        "  \n",
        "def alexnet32():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model31 = copy.deepcopy(model) \n",
        "  model31.features[0] = nn.Conv2d(3,110, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model31.features[3] = nn.Conv2d(110,200, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model31.features[6] = nn.Conv2d(200,192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model31.features[8] = nn.Conv2d(192,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model31\n",
        "  \n",
        "def alexnet33():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model32 = copy.deepcopy(model) \n",
        "  model32.features[0] = nn.Conv2d(3,100,kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model32.features[3] = nn.Conv2d(100,150, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model32.features[6] = nn.Conv2d(150,300,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model32.features[8] = nn.Conv2d(300,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model32\n",
        "  \n",
        "def alexnet34():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model33 = copy.deepcopy(model) \n",
        "  model33.features[0] = nn.Conv2d(3,80, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model33.features[3] = nn.Conv2d(80,150, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model33.features[6] = nn.Conv2d(150,200,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model33.features[8] = nn.Conv2d(200,256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model33\n",
        "  \n",
        "def alexnet35():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model34 = copy.deepcopy(model) \n",
        "  model34.features[0] = nn.Conv2d(3,150, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model34.features[3] = nn.Conv2d(150,230, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model34.features[6] = nn.Conv2d(230,395,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model34.features[8] = nn.Conv2d(395,256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model34\n",
        "  \n",
        "def alexnet36():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model35 = copy.deepcopy(model) \n",
        "  model35.features[0] = nn.Conv2d(3,160, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model35.features[3] = nn.Conv2d(160 ,195, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model35.features[6] = nn.Conv2d(195,240,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model35.features[8] = nn.Conv2d(240,256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model35\n",
        "  \n",
        "def alexnet37():\n",
        "  model  = torchvision.models.alexnet(pretrained=True)\n",
        "  model36 = copy.deepcopy(model) \n",
        "  model36.features[0] = nn.Conv2d(3,190, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "  model36.features[3] = nn.Conv2d(190,210, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "  model36.features[6] = nn.Conv2d(210,280,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model36.features[8] = nn.Conv2d(280,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model36\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "W9JJqO0Azqhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg1():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model1 = copy.deepcopy(model) \n",
        "  model1.features[3] = nn.Conv2d(64,150,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model1.features[6] = nn.Conv2d(150,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model1\n",
        "\n",
        "def vgg2():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)  \n",
        "  model2 = copy.deepcopy(model) \n",
        "  model2.features[3] = nn.Conv2d(64,180,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model2.features[6] = nn.Conv2d(180,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model2\n",
        "  \n",
        "def vgg3():\n",
        "  model  = torchvision.models.vgg11(pretrained=True) \n",
        "  model2 = copy.deepcopy(model) \n",
        "  model2.features[3] = nn.Conv2d(64,80,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model2.features[6] = nn.Conv2d(80,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model2\n",
        "  \n",
        "def vgg4():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model3 = copy.deepcopy(model) \n",
        "  model3.features[3] = nn.Conv2d(64,70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model3.features[6] = nn.Conv2d(70,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model3\n",
        "  \n",
        "def vgg5():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model4 = copy.deepcopy(model) \n",
        "  model4.features[3] = nn.Conv2d(64,105,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model4.features[6] = nn.Conv2d(105,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model4\n",
        "  \n",
        "def vgg6():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model5 = copy.deepcopy(model) \n",
        "  model5.features[3] = nn.Conv2d(64,120,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model5.features[6] = nn.Conv2d(120,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model5\n",
        "\n",
        "def vgg7():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model6 = copy.deepcopy(model) \n",
        "  model6.features[3] = nn.Conv2d(64,130, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model6.features[6] = nn.Conv2d(130,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model6\n",
        "  \n",
        "def vgg8():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model7 = copy.deepcopy(model) \n",
        "  model7.features[3] = nn.Conv2d(64,140, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model7.features[6] = nn.Conv2d(140,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model7\n",
        "  \n",
        "def vgg9():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model8 = copy.deepcopy(model) \n",
        "  model8.features[3] = nn.Conv2d(64,145, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model8.features[6] = nn.Conv2d(145,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model8\n",
        "  \n",
        "def vgg10():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model9 = copy.deepcopy(model) \n",
        "  model9.features[3] = nn.Conv2d(64,160,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model9.features[6] = nn.Conv2d(160,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model9\n",
        "  \n",
        "def vgg11():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model10 = copy.deepcopy(model) \n",
        "  model10.features[3] = nn.Conv2d(64,170,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model10.features[6] = nn.Conv2d(170,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model10\n",
        "  \n",
        "def vgg12():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)  \n",
        "  model11 = copy.deepcopy(model) \n",
        "  model11.features[3] = nn.Conv2d(64,150,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model11.features[6] = nn.Conv2d(150,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model11\n",
        "  \n",
        "def vgg13():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)  \n",
        "  model12 = copy.deepcopy(model) \n",
        "  model12.features[3] = nn.Conv2d(64,180,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model12.features[6] = nn.Conv2d(180,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model12\n",
        "  \n",
        "\n",
        "def vgg14():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model13 = copy.deepcopy(model) \n",
        "  model13.features[6] = nn.Conv2d(128,300,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model13.features[8] = nn.Conv2d(300,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model13\n",
        "  \n",
        "def vgg15():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model14 = copy.deepcopy(model) \n",
        "  model14.features[6] = nn.Conv2d(128,310,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model14.features[8] = nn.Conv2d(310,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model14\n",
        "  \n",
        "def vgg16():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model15 = copy.deepcopy(model) \n",
        "  model15.features[6] = nn.Conv2d(128,320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model15.features[8] = nn.Conv2d(320,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model15\n",
        "  \n",
        "def vgg17():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model16 = copy.deepcopy(model) \n",
        "  model16.features[6] = nn.Conv2d(128,330,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model16.features[8] = nn.Conv2d(330,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model16\n",
        "  \n",
        "def vgg18():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model17 = copy.deepcopy(model) \n",
        "  model17.features[6] = nn.Conv2d(128,340, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model17.features[8] = nn.Conv2d(340,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model17\n",
        "  \n",
        "def vgg19():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model18 = copy.deepcopy(model) \n",
        "  model18.features[6] = nn.Conv2d(128,350,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model18.features[8] = nn.Conv2d(350,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model18\n",
        "  \n",
        "def vgg20():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model19 = copy.deepcopy(model) \n",
        "  model19.features[6] = nn.Conv2d(128,360,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model19.features[8] = nn.Conv2d(360,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model19\n",
        "  \n",
        "def vgg21():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model20 = copy.deepcopy(model) \n",
        "  model20.features[6] = nn.Conv2d(128,370,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model20.features[8] = nn.Conv2d(370,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model20\n",
        "  \n",
        "def vgg22():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model21 = copy.deepcopy(model) \n",
        "  model21.features[6] = nn.Conv2d(128,380,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model21.features[8] = nn.Conv2d(380,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model21\n",
        "  \n",
        "def vgg23():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model22 = copy.deepcopy(model) \n",
        "  model22.features[6] = nn.Conv2d(128,200,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model22.features[8] = nn.Conv2d(200,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model22\n",
        "  \n",
        "def vgg24():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model23 = copy.deepcopy(model) \n",
        "  model23.features[6] = nn.Conv2d(128,210, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model23.features[8] = nn.Conv2d(210,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model23\n",
        "  \n",
        "def vgg25():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model24 = copy.deepcopy(model) \n",
        "  model24.features[6] = nn.Conv2d(128,250, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model24.features[8] = nn.Conv2d(250,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model24\n",
        "  \n",
        "def vgg26():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model25 = copy.deepcopy(model) \n",
        "  model25.features[6] = nn.Conv2d(128,220,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model25.features[8] = nn.Conv2d(220,256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model25  \n",
        "\n",
        "def vgg27():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model26 = copy.deepcopy(model)\n",
        "  model26.features[0] = nn.Conv2d(3 , 100,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model26.features[3] = nn.Conv2d(100,150,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model26.features[6] = nn.Conv2d(150,200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model26.features[8] = nn.Conv2d(200,256,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model26.features[11] = nn.Conv2d(256,280,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model26.features[13] = nn.Conv2d(280,300, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model26.features[16] = nn.Conv2d(300,350, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model26.features[18] = nn.Conv2d(350,512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model26  \n",
        "\n",
        "def vgg28():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model27 = copy.deepcopy(model)\n",
        "  model27.features[0] = nn.Conv2d(3 , 90,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model27.features[3] = nn.Conv2d(90,120,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model27.features[6] = nn.Conv2d(120,210, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model27.features[8] = nn.Conv2d(210,260,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model27.features[11] = nn.Conv2d(260,300,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model27.features[13] = nn.Conv2d(300,320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model27.features[16] = nn.Conv2d(320,380, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model27.features[18] = nn.Conv2d(380,512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model27\n",
        "  \n",
        "def vgg29():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model28 = copy.deepcopy(model)\n",
        "  model28.features[0] = nn.Conv2d(3 , 65,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model28.features[3] = nn.Conv2d(65,150,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model28.features[6] = nn.Conv2d(150,180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model28.features[8] = nn.Conv2d(180,210,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model28.features[11] = nn.Conv2d(210,250,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model28.features[13] = nn.Conv2d(250,290, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model28.features[16] = nn.Conv2d(290,400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model28.features[18] = nn.Conv2d(400,512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model28\n",
        "  \n",
        "def vgg30():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model29 = copy.deepcopy(model) \n",
        "  model29.features[11] = nn.Conv2d(256,300,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model29.features[13] = nn.Conv2d(300,512,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model29\n",
        "  \n",
        "\n",
        "def vgg31():\n",
        "  model  = torchvision.models.vgg11(pretrained=True)\n",
        "  model30 = copy.deepcopy(model) \n",
        "  model30.features[11] = nn.Conv2d(256,280,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model30.features[13] = nn.Conv2d(280,512,  kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "  return model30\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g3vh0Og_8yF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet1():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model1 = copy.deepcopy(model)\n",
        "  model1.layer1[0].conv1 = nn.Conv2d(64,128 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model1.layer1[0].bn1 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model1.layer1[0].conv2 = nn.Conv2d(128 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model1\n",
        "\n",
        "def resnet2():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model2 = copy.deepcopy(model)\n",
        "  model2.layer1[0].conv1 = nn.Conv2d(64,100 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model2.layer1[0].bn1 = nn.BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model2.layer1[0].conv2 = nn.Conv2d(100 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model2\n",
        "  \n",
        "def resnet3():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model3 = copy.deepcopy(model)\n",
        "  model3.layer1[0].conv1 = nn.Conv2d(64,110 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model3.layer1[0].bn1 = nn.BatchNorm2d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model3.layer1[0].conv2 = nn.Conv2d(110 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model3\n",
        "  \n",
        "def resnet4():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model4 = copy.deepcopy(model)\n",
        "  model4.layer1[0].conv1 = nn.Conv2d(64,180 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model4.layer1[0].bn1 = nn.BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model4.layer1[0].conv2 = nn.Conv2d(180 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model4\n",
        "  \n",
        "def resnet5():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model5 = copy.deepcopy(model)\n",
        "  model5.layer1[0].conv1 = nn.Conv2d(64,140 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model5.layer1[0].bn1 = nn.BatchNorm2d(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model5.layer1[0].conv2 = nn.Conv2d(140 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model5\n",
        "  \n",
        "def resnet6():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model6 = copy.deepcopy(model)\n",
        "  model6.layer1[1].conv1 = nn.Conv2d(64,120 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model6.layer1[1].bn1 = nn.BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model6.layer1[1].conv2 = nn.Conv2d(120 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model6\n",
        "  \n",
        "def resnet7():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model7 = copy.deepcopy(model)\n",
        "  model7.layer1[1].conv1 = nn.Conv2d(64,130 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model7.layer1[1].bn1 = nn.BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model7.layer1[1].conv2 = nn.Conv2d(130 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model7\n",
        "  \n",
        "def resnet8():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model8 = copy.deepcopy(model)\n",
        "  model8.layer1[1].conv1 = nn.Conv2d(64,100 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model8.layer1[1].bn1 = nn.BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model8.layer1[1].conv2 = nn.Conv2d(100 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model8\n",
        "  \n",
        "def resnet9():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model9 = copy.deepcopy(model)\n",
        "  model9.layer1[1].conv1 = nn.Conv2d(64,150 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model9.layer1[1].bn1 = nn.BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model9.layer1[1].conv2 = nn.Conv2d(150 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model9\n",
        "  \n",
        "def resnet10():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model10 = copy.deepcopy(model)\n",
        "  model10.layer1[1].conv1 = nn.Conv2d(64,200 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model10.layer1[1].bn1 = nn.BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model10.layer1[1].conv2 = nn.Conv2d(200 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model10\n",
        "  \n",
        "def resnet11():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model11 = copy.deepcopy(model)\n",
        "  model11.layer1[1].conv1 = nn.Conv2d(64,90 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  model11.layer1[1].bn1 = nn.BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model11.layer1[1].conv2 = nn.Conv2d(90 ,64 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model11\n",
        "  \n",
        "\n",
        "def resnet12():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model12 = copy.deepcopy(model)\n",
        "  model12.layer2[0].conv1 = nn.Conv2d(64, 100, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "  model12.layer2[0].bn1 = nn.BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model12.layer2[0].conv2 = nn.Conv2d(100 ,128 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model12\n",
        "  \n",
        "def resnet13():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model13 = copy.deepcopy(model)\n",
        "  model13.layer2[0].conv1 = nn.Conv2d(64, 80 ,  kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "  model13.layer2[0].bn1 = nn.BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model13.layer2[0].conv2 = nn.Conv2d(80 ,128 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model13\n",
        "  \n",
        "\n",
        "def resnet14():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model11 = copy.deepcopy(model)\n",
        "  model11.layer2[0].conv1 = nn.Conv2d(64,300 ,  kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "  model11.layer2[0].bn1 = nn.BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model11.layer2[0].conv2 = nn.Conv2d(300 ,128 , kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "  return model11\n",
        "   \n",
        "\n",
        "def resnet15():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model12 = copy.deepcopy(model)\n",
        "  model12.layer4[0].conv1 = nn.Conv2d(256, 400, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "  model12.layer4[0].bn1 = nn.BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model12.layer4[0].conv2 = nn.Conv2d(400, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  return model12\n",
        "  \n",
        "def resnet16():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model13 = copy.deepcopy(model)\n",
        "  model13.layer4[0].conv1 = nn.Conv2d(256, 300, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "  model13.layer4[0].bn1 = nn.BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model13.layer4[0].conv2 = nn.Conv2d(300, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  return model13\n",
        "  \n",
        "\n",
        "def resnet17():\n",
        "  model =  torchvision.models.resnet18(pretrained=True)\n",
        "  model14 = copy.deepcopy(model)\n",
        "  model14.layer4[0].conv1 = nn.Conv2d(256, 500, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "  model14.layer4[0].bn1 = nn.BatchNorm2d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  model14.layer4[0].conv2 = nn.Conv2d(500, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "  return model14\n",
        "  \n"
      ],
      "metadata": {
        "id": "1XOiW5j0TRMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alexnet():\n",
        "  return torchvision.models.alexnet(pretrained=True)\n",
        "\n",
        "def vg11():\n",
        "  return torchvision.models.vgg11(pretrained=True)\n",
        "\n",
        "def vgg11_bn():\n",
        "  return torchvision.models.vgg11_bn(pretrained=True)\n",
        "\n",
        "def vgg13():\n",
        "  return torchvision.models.vgg13(pretrained=True)\n",
        "\n",
        "def vgg13_bn():\n",
        "  return torchvision.models.vgg13_bn(pretrained=True)\n",
        "\n",
        "def vgg16():\n",
        "  return torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "def vgg16_bn():\n",
        "  return torchvision.models.vgg16_bn(pretrained=True)\n",
        "\n",
        "def vgg19():\n",
        "  return torchvision.models.vgg19(pretrained=True)\n",
        "\n",
        "def vgg19_bn():\n",
        "  return torchvision.models.vgg19_bn(pretrained=True)\n",
        "\n",
        "def resnet18():\n",
        "  return torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "def resnet34():\n",
        "  return torchvision.models.resnet34(pretrained=True)\n",
        "\n",
        "def resnet50():\n",
        "  return torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "def resnet101():\n",
        "  return torchvision.models.resnet101(pretrained=True)\n",
        "\n",
        "def resnet152():\n",
        "  return torchvision.models.resnet152(pretrained=True)\n",
        "\n",
        "def densenet121():\n",
        "  return torchvision.models.densenet121(pretrained=True)\n",
        "\n",
        "def densenet169():\n",
        "  return torchvision.models.densenet169(pretrained=True)\n",
        "\n",
        "def densenet161():\n",
        "  return torchvision.models.densenet161(pretrained=True)\n",
        "\n",
        "def densenet201():\n",
        "  return torchvision.models.densenet201(pretrained=True)\n",
        "\n",
        "def googlenet():\n",
        "  return torchvision.models.googlenet(pretrained=True)\n",
        "\n",
        "def wide_resnet50_2():\n",
        "  return torchvision.models.wide_resnet50_2(pretrained=True)\n",
        "\n",
        "def wide_resnet101_2():\n",
        "  return torchvision.models.wide_resnet101_2(pretrained=True)\n"
      ],
      "metadata": {
        "id": "1P3xvXLEoLmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModule(nn.Module):\n",
        "    def __init__(self , config_list):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.convList = nn.ModuleList()\n",
        "        for i in range(len(config_list) -1):\n",
        "          self.convList.append(nn.Conv2d(config_list[i] , config_list[i+1] , (3 , 3) , 1 ,1))\n",
        "          self.convList.append(nn.ReLU(inplace=True))\n",
        "        self.adap_pool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
        "        input_size = config_list[-1] * 7 * 7\n",
        "        self.classifier = nn.Sequential(nn.Linear(input_size , 4096 , bias=True) , nn.ReLU(inplace=True) , nn.Linear(4096 , 4096 , bias=True) , nn.ReLU(inplace=True),nn.Linear(4096,1000 , bias=True) )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.convList)):\n",
        "            x = self.convList[i](x)\n",
        "        x = self.adap_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_random_list():\n",
        "  import random\n",
        "  random.seed(0)\n",
        "  final_list = []\n",
        "  for i in range(2 , 200):\n",
        "    l = [3]\n",
        "    for j in range(i):\n",
        "      l.append(random.randint(3, 50))\n",
        "    final_list.append(l)\n",
        "    l = [3]\n",
        "    for j in range(i):\n",
        "      l.append(random.randint(3, 50))\n",
        "    final_list.append(l)\n",
        "    l = [3]\n",
        "    for j in range(i):\n",
        "      l.append(random.randint(3, 50))\n",
        "    final_list.append(l)\n",
        "    l = [3]\n",
        "    for j in range(i):\n",
        "      l.append(random.randint(3, 50))\n",
        "    final_list.append(l)\n",
        "    l = [3]\n",
        "    for j in range(i):\n",
        "      l.append(random.randint(3, 50))\n",
        "    final_list.append(l)\n",
        "  return final_list\n",
        "\n",
        "def architecture_by_configuration(config_list):\n",
        "  architecture = MyModule(config_list)\n",
        "  return architecture\n"
      ],
      "metadata": {
        "id": "RBPF6QPJ9oCc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hardware_latency(hardware_name , hardware_info , if_hardware_is_cuda):\n",
        "  models = [\"alexnet\" ,\"vgg11\" , \"vgg11_bn\" ,\"vgg13\", \"vgg13_bn\" , \"vgg16\" , \"vgg16_bn\" , \"vgg19\" , \"vgg19_bn\" , \"resnet18\" , \"resnet34\" , \"resnet50\" , \"resnet101\" , \"resnet152\" , \"densenet121\" , \"densenet169\" , \"densenet161\" , \"densenet201\" , \"googlenet\" ,\"wide_resnet50_2\" , \"wide_resnet101_2\"]\n",
        "  for i in range(1 , 38):\n",
        "    models.append(\"alexnet{}\".format(i))\n",
        "  for i in range(1 , 32):\n",
        "    models.append(\"vgg{}\".format(i))\n",
        "  for i in range(1 , 18):\n",
        "    models.append(\"resnet{}\".format(i))\n",
        "\n",
        "  latency_list = []\n",
        "  std_dev_list = []\n",
        "  for model_string in models:\n",
        "    model = eval(model_string + \"()\")\n",
        "    print(\"Loaded {}\".format(model_string))\n",
        "    dummy_input = generate_input()\n",
        "    print(\"Calculating latency for {}\".format(model_string))\n",
        "    if if_hardware_is_cuda:\n",
        "      latency_model , std_dev = calculate_latency_gpu(model , dummy_input)\n",
        "    else:\n",
        "      latency_model , std_dev = calculate_latency_cpu(model , dummy_input)\n",
        "      \n",
        "    latency_list.append(latency_model)\n",
        "    std_dev_list.append(std_dev)\n",
        "  \n",
        "  random_list = get_random_list()\n",
        "  index_list = list(range(len(random_list)))\n",
        "  print(len(index_list))\n",
        "  for index in index_list:\n",
        "    config_list = random_list[index]\n",
        "    model = architecture_by_configuration(config_list)\n",
        "    print(\"Loaded {}\".format(index))\n",
        "    dummy_input = generate_input()\n",
        "    print(\"Calculating latency for {}\".format(index))\n",
        "    if if_hardware_is_cuda:\n",
        "      latency_model , std_dev = calculate_latency_gpu(model , dummy_input)\n",
        "    else:\n",
        "      latency_model , std_dev = calculate_latency_cpu(model , dummy_input)\n",
        "      \n",
        "    latency_list.append(latency_model)\n",
        "    std_dev_list.append(std_dev)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  hardware_dict = {}\n",
        "  hardware_dict[\"name\"] = hardware_name\n",
        "  hardware_dict[\"hardware_info\"] = hardware_info \n",
        "  hardware_dict[\"latency\"] = latency_list\n",
        "  hardware_dict[\"std_dev_latency\"] = std_dev_list\n",
        "  torch.save(hardware_dict , hardware_name+\".pt\")\n",
        "\n",
        "  print(\"Calculated hardware latency\")\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "lsR9h58MoZcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_hardware_latency(\"K80\" , \"No Info\" , True)"
      ],
      "metadata": {
        "id": "dgdspR8Dq6SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = architecture_by_configuration(get_random_list()[-1])\n",
        "a = generate_input()\n",
        "calculate_latency_gpu(model , a)"
      ],
      "metadata": {
        "id": "_TTuqL-VajOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d206cc-fcc9-460a-a6b4-26df250faba9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1085.3211653645833, 4.098043575205824)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "prijmZ5mb3cO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}